---
layout: post
title: The non-linear neural network function
---

if linear functions are \\(s = W x\\), then a 2-layer neural network function would look more like look more like:
$$ s = W_2 max(0, W_1 x) $$

Where \\(W_1\\) has the dimensions of input and an intermediate dimension and \\(W_2\\) mapping from the intermediate dimension to our output dimension. Notice that if we remove the element-wise \\(max\\) function then we would be able to collapse \\(W_2 W_1\\) into a single matrix.

Similarly, a three-layer neural network would look like:
$$ s = W_3 max(0, W_2 max(0, W_1 x)) $$

With the biological tilt, dendrites carry signal to a neuron's cell body where the signal accumulates. If the final signal sums above a certain threshold, the neuron 'fire, sending a spike along its axon. In the computational model, we assume that the precise timings of the spikes do not matter, and that only the frequency of the firing communicates information. Based on this rate code interpretation, we model the firing rate of the neuron with an activation function.

Activations functions are incredibly versitile. With the simple notion that a neuron can "like" (have an activation weight near 1.0) or "dislike" (a weight near 0.0) an input, we can encode a binary classifier (interpret activation as one class and `1-activation` as the contrasting class), a binary SVM classifier (attaching a max-margin hinge loss as the activation), or we can interpret the activation as kind of regularization (from a biological perspective: how easy it is to "gradually forget" a parameter).

Common activation function include the sigmoid function (which squashes numbers between [0,1]), tanh (which squashes numbers to the range of [-1,1]), relu (rectified linear units: which max(0,x) numbers), leaky relu, and maxout.

---

Sigmoid activation has fallen out of favor as an intermediate layer since they tend to saturate at either the tail of 1 or 0, which cause the gradient at those regions to zero-out. Gradients of neglegent magnitude fail to learning via backpropagation, blocking any updates from flowing through the network. On top of this, sigmoid outputs are not centered around zero. I think the reasoning behind how this is bad may be best explained in the Stanford CS231 course's page:

> This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. \\(x > 0\\) elementwife in \\(f = w^T x + b \\)), then the gradient on the weights \\(w\\) will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression \\(f\\)). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are
> added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation probjem above.

The zig-zagging can be explained from the fact that a update may have to overcompensate for a negative value in a positive space when, later, a subsequent update will have to, again, overcomensate for a poor update. Tanh also suffers from problems of saturated gradients, but it is always preferred to sigmoid activation because of the latter issue.

ReLU (Rectified Linear Units) are probably the most popular at the time of this writing. 
