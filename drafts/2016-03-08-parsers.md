---
date: 1900-01-01
---


parseing has two stages: lexing and parsing

tokenization is a parsing tactic. 

characters from a stream are fed to a lexer, which then emits tokens to the parser.

the parser structures the tokens from the stream into a tree which is what is usually
called an "abstract syntax tree" (more commonly, AST).

Lexers are simple. usually they don't require more than a one-step lookahead.
Sometimes you'll hear them called "tokenizers" - this can sometimes be done with
regex - but the main point here is that you're just figuring out what syntax is
valid.

lexers and parsers are primarily different based on how complex the grammar is.

[0]: https://en.wikipedia.org/wiki/Chomsky_hierarchy


